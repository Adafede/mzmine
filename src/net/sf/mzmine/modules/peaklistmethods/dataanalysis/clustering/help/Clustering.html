<html>
        <head>
                <title>Data analysis - Clustering</title>
                <meta http-equiv="Content-Type" content="text/html; charset=UTF-8" />
                <link rel="stylesheet" type="text/css" href="/net/sf/mzmine/desktop/impl/helpsystem/HelpStyles.css">
        </head>

        <body>

                <h1>Clustering</h1>

                <h2>Description</h2>
                <p>
                        The goal of clustering is to group a set of observations into subsets (clusters) finding an intrinsic structure in them so that the members of one single cluster share more similarities than the members of distinct clusters. Clustering is the most important method of unsupervised learning, and a common technique for statistical data analysis used in many fields.
                        There is no absolute "best" criterion which would be independent of the final aim of the clustering. Consequently, it is the user which must supply this criterion, in such a way that the result of the clustering will suit their needs.
                        <br><br>
                        The result of the clustering can be visualized using PCA plot of Sammon's projection, and the data can be seen in a table where in the first column are the names of the samples or variables and in the second column the cluster id (a number) for each sample or variable. Visualization of the hierarchical clustering result is implemented based on TreeViewJ software ("Peterson, M.W. and M.E. Colosimo, TreeViewJ: an application for viewing and analysing phylogenetic trees. Source Code Biol Med, 2007. 2(1): p. 7.")
                </p>
                <p>
                        <img src="TreeView.png" name="Tree View">
                </p>

                <h3>Density Based Clustering using EM algorithm</h3>
                <p>
                        Each cluster is assumed to have a probability density with certain parameters (e.g. Multivariate Gaussian). The goal of Density Based clustering is to determine the number of such model components (i.e. clusters) in a data set, and the parameters of the probability density of each component. Once the components of the whole data set are determined, a Density Based cluster may indicate the probability of each variable belonging to a particular cluster. Number of clusters is determined using cross-validation. Each variable has a probability distributiona indicating the probability of the variable belonging to each of the clusters.
                </p>


                <h3>Simple K-Means</h3>

                <p>
                        The goal of K-means clustering is to determine k clusters in such a way that intra cluster distances are small and inter cluster distances are large; or in other words, every point is assigned to a cluster whose centre is the nearest. K-means clustering works by randomly choosing k-centroids in the first step and then assigning the data points to the clusters in such a way that every point belongs to the cluster with the nearest centroid, and redetermining the cluster centroids by taking the mean of data points in each cluster. The process is continued until the cluster means converge.
                </p>

                <h3>Farthest First algorithm for clustering</h3>

                <p>
                        Farthest first is an algorithm to choose the cluster centers in K-means clustering. It works by placing each cluster centre in turn at the point furthest from the existing cluster centres. This point must lie within the data area. This greatly speeds up the clustering in most cases since less reassignment and adjustment are needed.
                </p>

                <h3>Hierarchical clustering</h3>
                <p>
                        Hierarchical clustering builds a hierarchy of clusters. It is either achieved using Agglomerative clustering, in which initially every point belongs to a distinct cluster and the clusters are combined with the nearest clusters iteratively; or by dividing clusters (Divisive) starting from one single cluster containing all data points, until every singe point belongs to a separate cluster. The distances between points maybe determined using e.g. Euclidean, Minkowski or Manhattan distance; and the distances between clustered maybe determined by single linkage (minimum distance between all pairs of points between the clusters), complete linkage (maximum distance between all pairs of points between clusters), and so on. Determining the number of clusters is done by setting a length to "cut" the hierarchical clustering tree, but hierarchical clustering is more commonly used as a tool for visualizing the patterns of neighbourhood.
                </p>



                <h4>Method parameters</h4>
                <dl>
                        <dt>Data files</dt>
                        <dd>Raw data files correspondent to the samples selected to bi in the projection plot.</dd>

                        <dt>Colouring style</dt>
                        <dd>The dots corresponding to every sample can be colored depending on the sample's parameter state or on the file.</dd>

                        <dt>Peak measuring approach</dt>
                        <dd>It can take two values: height or area. The projections will be calculated using one of this two values.</dd>

                        <dt>Peaks</dt>
                        <dd>Peaks that will be taken into account to create the projection plot.</dd>

                        <dt>Visualization</dt>
                        <dd>The visualization of the result of non hierarchical clustering algorithms can be performed using PCA or Sammon's projection</dd>

                        <dt>Type of data</dt>
                        <dd>It can take two values: Samples or variables. The clustering will be applied to one of this types of data.</dd>

                        <dt>Algorithm</dt>
                        <dd>Algorithm that will be used to cluster the data.</dd>

                        <dt>Link type</dt>
                        <dd>This parameters is only enable when the hierarchical clustering has been chosen. The distances between clusters is determined by the chosen linkage.</dd>

                        <dt>Distance fuction</dt>
                        <dd>This parameters is only enable when the hierarchical clustering has been chosen. The distances between points is determined by the chosen distance function. </dd>

                        <dt>Number of groups</dt>
                        <dd>The number of clusters has to be defined by the user in advance for some clustering algorithms. This parameter is available only when K-means or Farthest First algorithm are chosen. </dd>
                </dl>

        </body>
</html>
